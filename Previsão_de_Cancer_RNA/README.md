## Machine Learning 



```python
# Vers√£o da Linhga Python
from platform import python_version
print("Ver~sao da Linguagem Python usada neste Jupyter Notebook: ", python_version())
```

    Ver~sao da Linguagem Python usada neste Jupyter Notebook:  3.8.8


## A Matem√°tica das Redas Neurais Artificiais
Construindo a Rede Neural com Programa√ß√£o e Matem√°tica

![title](imagens/rna.png)

#### Etapas:

1 - Construir uma rede neural artificial somente com opera√ß√µes matem√°ticas;

2 - Treinar a rede para Prever a Ocorr√™ncia de C√¢ncer;

### A Arquitetura de Redes Neurais Articiais

Uma rede neural t√≠pica √© constitu√≠da por um conjunto de neur√¥nios interligados, infuenciando uns aos outros formando um sistema maior, capaz de armazenar conhecimento adquirido por meio de exemplos apresentados e, assim, podendo realizar infer√™ncias sobre novos conjuntos de dados. Vejamos a arquitetura de redes neurais artificiais.

As redes neurais s√£o comumente apresentadas como um grafo orientado, onde os v√©rtices s√£o os neur√¥nios e as arestas as sinapses. A dire√ß√£o das arestas informa o tipo de alimenta√ß√£o, ou seja, como os neur√¥nios s√£o alimentados (recebem sinais de entrada). As redes neurais derivam seu poder devido a sua estrutura massiva e paralela e a habilidade de aprender por experi√™ncia. Essa experi√™ncia √© transmitida por meio de exemplos obtidos do mundo real, definidos como um conjunto de caracter√≠sticas formados por dados de entrada e de sa√≠da. Se apresentamos esses dados de entrada e sa√≠da √† rede, estamos diante de aprendizagem supervsionada e caso apresentemos apenas os dados de entrada, estamos diante de aprendizagem n√£o supervisionada!

O conhecimento obtido pela rede atrav√©s dos exemplos √© armazenado na forma de pesos das conex√µes, os quais ser√£o ajustados a fim de tomar decis√µes corretas a partir de novas entradas, ou seja, novas situa√ß√µes do mundo real n√£o conhecidas pela rede. O processo de ajuste dos pesos sinapticos √© realizado pelo algoritmo de aprendizagem, respons√°vel em armazenar na rede o conhecimento do mundo real obtido atraves de exemplos. Existem v√°rios algoritmos de aprendizagem, dentre eles o backpropagation que √© o algoritmo mais utilizado.

![title](imagens/nnet.png)


```python
# Por enquanto precisaremos somente do NumPy
import numpy as np
```


```python
# Instala o pacote watermark. 
# Esse pacote √© usado para gravar as vers√µes de outros pacotes usados neste jupyter notebook.
!pip install -q -U watermark
```


```python
# Vers√µes dos pacotes usados neste jupyter notebook
%reload_ext watermark
%watermark -a "Rodolfo Terra | Ci√™ncia de Dados" --iversions
```

    Author: Rodolfo Terra | Ci√™ncia de Dados
    
    numpy: 1.19.2


‚Äã    

## 1.0. Implementando Uma Rede Neural Artificial Somente com F√≥rmulas Matem√°ticas (Sem Frameworks)

### 1.1. Forward Propagation

![title](imagens/nn.png)

#### 1.1.1 Desenvolvendo a Fun√ß√£o Para Inicializa√ß√£o de Pesos


```python
# Fun√ß√£o para inicializa√ß√£o rand√¥mica dos par√¢metros do modelo
def inicializa_parametros(dims_camada_entrada):
    
    # Dicion√°rio para os par√¢metros
    parameters = {}
    
    # Comprimento das dimens√µes das camadas
    comp = len(dims_camada_entrada)
    
    # Loop pelo comprimento
    for i in range(1, comp):
        
        # Inicializa√ß√£o da matriz de pesos
        parameters["W" + str(i)] = np.random.randn(dims_camada_entrada[i], dims_camada_entrada[i - 1]) * 0.01
        
        # Inicializa√ß√£o do bias
        parameters["b" + str(i)] = np.zeros((dims_camada_entrada[i], 1))
    
    return parameters
```

#### 1.1.2. Desenvolvendo a Fun√ß√£o Sigmoide

A principal raz√£o pela qual usaremos a fun√ß√£o sigm√≥ide √© porque ela permite converter n√∫meros para valores entre 0 e 1.

Portanto √© especialmente usada para modelos em que temos que prever probabilidade como uma sa√≠da. Com a probabilidade de qualquer coisa existrir apenas entre o intervalo de 0 e 1, sigm√≥ide √© a escolha certa. Algumas caracter√≠sticas da fun√ß√£o sigm√≥ide:

* A fun√ß√£o √© diferenci√°vel. Isso significa que podemos encontrar a inclina√ß√£o da curva sgm√≥ide em dois pontos;

* A fun√ß√£o sigm√≥ide log√≠stica pode fazer com que uma rede neural fique presa no momento do treinamento;

* A fun√ß√£o softmax √© uma fun√ß√£o de ativa√ß√£o log√≠stica mais generalizada, utilizada para a classifica√ß√£o em v√°rias classes. 

![title](imagens/sigmoid.png)

Se a fun√ß√£o parecer muito abstrata ou estranha para voc√™, n√£o se preocupe muito com detalhes como o n√∫mero de Euler e ou como algu√©m criou essa fun√ß√£o. Para aqueles que n√£o s√£o conhecedores de matem√°tica, a √∫nica coisa importante sobre a fun√ß√£o sigm√≥ide √© primeiro, sua curva e, segundo, sua derivada. Aqui est√£o mais alguns detalhes:

- **A fun√ß√£o sigm√≥ide produz resultados semelhantes aos da fun√ß√£o de passo (Step Function) em que a sa√≠da est√° entre 0 e 1. A curva cruza 0,5 a z = 0, e podemos definir regras para a fun√ß√£o de ativa√ß√£o, como: Se a sa√≠da do neur√¥nio sigm√≥ide for maior que ou igual a 0,5, gera 1; se a sa√≠da for menor que 0,5, gera 0.**


- A fun√ß√£o sigm√≥ide √© suave e possui uma derivada simples de œÉ(z) * (1 - œÉ (z)), que √© diferenci√°vel em qualquer lugar da curva. 


- Se z for muito negativo, a sa√≠da ser√° aproximadamente 0; se z for muito positivo, a sa√≠da √© aproximadamente 1; mas em torno de z = 0, onde z n√£o √© muito grande nem muito pequeno, temos um desvio relativamente maior √† medida que z muda.

#### 1.1.3. Afinal, O Que √© Derivada?

![title](imagens/derivada.png)

No C√°lculo, a derivada em um ponto de uma fun√ß√£o y = f(x) representa a taxa de varia√ß√£o instant√¢nea de y em rela√ß√£o a x neste ponto. 

Um exemplo t√≠pico √© a fun√ß√£o velocidade que representa a taxa de varia√ß√£o (derivada) da fun√ß√£o espa√ßo. Do mesmo modo, a fun√ß√£o acelera√ß√£o √© a derivada da fun√ß√£o velocidade. Geometricamente, a derivada no ponto x = a de y = f(x) representa a inclina√ß√£o da reta tangente ao gr√°fico desta fun√ß√£o no ponto (a, f(a)).

A fun√ß√£o que a cada ponto x associa a derivada neste ponto de f(x) √© chamada de fun√ß√£o derivada de f(x).

![title](imagens/derivada.gif)

Em cada ponto, a derivada de f(x) √© a tangente do √¢ngulo que a reta tangente √† curva faz em rela√ß√£o ao eixo das abscissas. A reta √© sempre tangente √† curva azul; a tangente do √¢ngulo que ela faz com o eixo das abscissas √© a derivada. Note-se que a derivada √© positiva quando verde, negativa quando vermelha, e zero quando preta.

A derivada de uma fun√ß√£o y = f(x) num ponto x = x0, √© igual ao valor da tangente trigonom√©trica do √¢ngulo formado pela tangente geom√©trica √† curva representativa de y=f(x), no ponto x = x0, ou seja, a derivada √© o coeficiente angular da reta tangente ao gr√°fico da fun√ß√£o no ponto x0.

A fun√ß√£o derivada √© representada por f'(x).


```python
# Fun√ß√£o sigm√≥ide
def sigmoid(Z):
    A = 1 / (1 + np.exp(-Z))
    return A, Z
```

#### 1.1.4. Desenvolvendo a Fun√ß√£o ReLu

Para usar a descida de gradiente estoc√°stico com retropropaga√ß√£o de erros para treinar redes neurais profundas, √© necess√°ria uma fun√ß√£o de ativa√ß√£o que se assemelhe e atue como uma fun√ß√£o linear, mas √©, de fato, uma fun√ß√£o n√£o linear que permite que relacionamentos complexos nos dados sejam aprendidos.

A solu√ß√£o √© usar a fun√ß√£o de ativa√ß√£o linear retificada ou ReL para abreviar. Um n√≥ ou unidade que implementa essa fun√ß√£o de ativa√ß√£o √© chamado de unidade de ativa√ß√£o linear retificada ou ReLU, para abreviar. Frequentemente, as redes que usam a fun√ß√£o retificadora para as camadas ocultas s√£o chamadas de redes retificadas.

A fun√ß√£o ReLU √© definida como ùëì(ùë•) = max (0, ùë•). Normalmente, ela √© aplicada elemento a elemento √† sa√≠da de alguma outra fun√ß√£o, como um produto de vetor e matriz. 

A ado√ß√£o da ReLU pode ser facilmente considerada um dos marcos na revolu√ß√£o do aprendizado profundo, por ex. as t√©cnicas que agora permitem o desenvolvimento rotineiro de redes neurais muito profundas.

A derivada da fun√ß√£o linear retificada tamb√©m √© f√°cil de calcular. **A derivada da fun√ß√£o de ativa√ß√£o √© necess√°ria ao atualizar os pesos de um n√≥ como parte da retropropaga√ß√£o de erro.**

A derivada da fun√ß√£o √© a inclina√ß√£o. A inclina√ß√£o para valores negativos √© 0,0 e a inclina√ß√£o para valores positivos √© 1,0.

Tradicionalmente, o campo das redes neurais evitou qualquer fun√ß√£o de ativa√ß√£o que n√£o fosse completamente diferenci√°vel, talvez adiando a ado√ß√£o da fun√ß√£o linear retificada e de outras fun√ß√µes lineares. Tecnicamente, n√£o podemos calcular a derivada quando a entrada √© 0,0; portanto, podemos assumir que √© zero. Este n√£o √© um problema na pr√°tica.

Os gradientes das ativa√ß√µes tangentes e hiperb√≥licas s√£o menores que a por√ß√£o positiva da ReLU. Isso significa que a parte positiva √© atualizada mais rapidamente √† medida que o treinamento avan√ßa. No entanto, isso tem um custo. O gradiente 0 no lado esquerdo tem seu pr√≥prio problema, chamado "neur√¥nios mortos", no qual uma atualiza√ß√£o de gradiente define os valores recebidos para uma ReLU, de modo que a sa√≠da √© sempre zero; unidades ReLU modificadas, como ELU (ou Leaky ReLU, ou PReLU, etc.) podem melhorar isso.

![title](imagens/relu.png)


```python
# Fun√ß√£o de ativa√ß√£o ReLu (Rectified Linear Unit)
def relu(Z):
    A = abs(Z * (Z > 0))
    return A, Z
```

![title](imagens/net-relu.png)


#### 1.1.5. Desenvolvendo a Ativa√ß√£o Linear


```python
# Opera√ß√£o de ativa√ß√£o
# A √© a matriz com os dados de entrada
# W √© a matriz de pesos
# b √© o bias
def linear_activation(A, W, b):
    Z = np.dot(W, A) + b
    cache = (A, W, b)
    return Z, cache
```

#### 1.1.6. Construindo o Proceso de Forward Propagation


```python
# Movimento para frente (forward)
def forward(A_prev, W, b, activation):
    
    # Se a fun√ß√£o de ativa√ß√£o for Sigmoid, entramos neste bloco
    if activation == "sigmoid":
        Z, linear_cache = linear_activation(A_prev, W, b)
        A, activation_cache = sigmoid(Z)
        
    # Se n√£o, se for ReLu, entramos neste bloco    
    elif activation == "relu":
        Z, linear_cache = linear_activation(A_prev, W, b)
        A, activation_cache = relu(Z)
        
    cache = (linear_cache, activation_cache)
    
    return A, cache
```

#### 1.1.7. Combinando Ativa√ß√£o e Propaga√ß√£o


```python
# Propaga√ß√£o para frente
def forward_propagation(X, parameters):
    
    # Lista de valores anteriores (cache)
    caches = []
    
    # Dados de entrada
    A = X
    
    # Comprimento dos par√¢metros
    L = len(parameters) // 2
   
    # Loop
    for i in range(1, L):
      
        # Guarda o valor pr√©vio de A
        A_prev = A
        
        # Executa o forward
        A, cache = forward(A_prev, parameters["W" + str(i)], parameters["b" + str(i)], activation = "relu")
        
        # Grava o cache
        caches.append(cache)
    
    # Sa√≠da na √∫ltima camada
    A_last, cache = forward(A, parameters["W" + str(L)], parameters["b" + str(L)], activation = "sigmoid")
    
    # Grava o cache
    caches.append(cache)
    
    return(A_last, caches)
```

#### 1.1.8. Desenvolvendo a Fun√ß√£o de Custo

![title](imagens/custo.png)


```python
# Fun√ß√£o de custo (ou fun√ß√£o de erro)
def calcula_custo(A_last, Y):
    
    # Ajusta o shape de Y para obter seu comprimento (total de elementos)
    m = Y.shape[1]
    
    # Calcula o custo comparando valor real e previso
    custo = (-1 / m) * np.sum((Y * np.log(A_last)) + ((1 - Y) * np.log(1 - A_last)))
    
    # Ajusta o shape do custo
    custo = np.squeeze(custo)
    
    return(custo)
```

## 1.2 Backward Propagation


![title](imagens/backpropagation.png)


#### 1.2.1. Por Que Precisamos deBackpropagation (Retropropaga√ß√£o)?

Ao projetar uma rede neural, primeiro, precisamos treinar um modelo e atribuir pesos espec√≠ficos a cada uma das entradas. Esse peso decide o qu√£o vital √© esse recurso para nossa previs√£o. Quanto maior o peso, maior a import√¢ncia. No entanto, inicialmente, n√£o sabemos o peso espec√≠fico exigido pelas entradas. Ent√£o, o que fazemos √© atribuir um peso aleat√≥rio √†s nossas entradas e nosso modelo calcula o erro na previs√£o. Depois disso, atualizamos nossos valores de peso e executamos novamente o c√≥digo (retropropaga√ß√£o). Ap√≥s v√°rias itera√ß√µes, podemos obter valores de erro mais baixos e maior precis√£o.

Para que a retropropaga√ß√£o funcione usamos um algoritmo que verifica quanto o valor de  cada  peso  afeta  o  erro  do  modelo,  calculando  as  derivadas  parciais. Esse  algoritmo  √© chamado de Gradiente Descendente e √© aplicado atrav√©s da Chain Rule (Regra da Cadeia).

Em outras palavras, a retropropaga√ß√£o visa minimizar a fun√ß√£o de custo ajustando os pesos e vieses (bias)da rede. O n√≠vel de ajuste √© determinado pelos gradientes da fun√ß√£o de custo em rela√ß√£o a esses par√¢metros.Uma pergunta pode surgir:por que calcular gradientes?O gradiente de uma fun√ß√£o C(x_1, x_2,..., x_m) no ponto x √© um vetor das derivadas parciais de C em x.A derivada de uma fun√ß√£o C mede a sensibilidade √† altera√ß√£o do valor da fun√ß√£o (valor de sa√≠da) em rela√ß√£o a uma altera√ß√£o no argumento x(valor de entrada). Em outras palavras, a derivada nos diz a dire√ß√£o que C est√° seguindo.O  gradiente  mostra  quanto  o  par√¢metro  x  precisa  mudar  (na  dire√ß√£o  positiva  ou negativa) para minimizar C. O c√°lculo desses gradientes acontece usando uma t√©cnica chamada Regra da Cadeia.



#### 1.2.2. Desenvolvendo o Backward Propagation - Fun√ß√£o Sigm√≥ide Backward


```python
# Fun√ß√£o sigmoid para o backpropagation 
# Fazemos o c√°lculo da derivada pois n√£o queremos o valor completo da fun√ß√£o, mas sim sua varia√ß√£o
def sigmoid_backward(da, Z):
    
    # Calculamos a derivada de Z
    dg = (1 / (1 + np.exp(-Z))) * (1 - (1 / (1 + np.exp(-Z))))
    
    # Encontramos a mudan√ßa na derivada de z
    dz = da * dg
    return dz

# Compare com a fun√ß√£o sigmoid do forward propagation
# A = 1 / (1 + np.exp(-Z))
```

#### 1.2.3 Desenvolvendo o Backward Propagation - Fun√ß√£o ReLu Backward


```python
# Fun√ß√£o relu para o backpropagation 
# Fazemos o c√°lculo da derivada pois n√£o queremos o valor completo da fun√ß√£o, mas sim sua varia√ß√£o
def relu_backward(da, Z):
    
    dg = 1 * ( Z >= 0)
    dz = da * dg
    return dz

# Compare com a fun√ß√£o relu do forward propagation:
# A = abs(Z * (Z > 0))
```

#### 1.2.4. Ativa√ß√£o Linear backward

##### 1.2.4.1 Desenvolvendo o Backward Propagation


```python
# Ativa√ß√£o linear para o backpropagation
def linear_backward_function(dz, cache):
    
    # Recebe os valores do cache (mem√≥ria)
    A_prev, W, b = cache
    
    # Shape de m
    m = A_prev.shape[1]
    
    # Calcula a derivada de W (resultado da opera√ß√£o com dz)
    dW = (1 / m) * np.dot(dz, A_prev.T)
    
    # Calcula a derivada de b (resultado da opera√ß√£o com dz)
    db = (1 / m) * np.sum(dz, axis = 1, keepdims = True)
    
    # Calcula a derivada da opera√ß√£o
    dA_prev = np.dot(W.T, dz)
    
    return dA_prev, dW, db
```

##### 1.2.4.2. Desenvolvendo o Backward Propagation


```python
# Fun√ß√£o que define o tipo de ativa√ß√£o (relu ou sigmoid)
def linear_activation_backward(dA, cache, activation):
    
    # Extrai o cache
    linear_cache, activation_cache = cache
    
    # Verifica se a ativa√ß√£o √© relu
    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)
        
    # Verifica se a ativa√ß√£o √© sigmoid
    if activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)
        
    return dA_prev, dW, db
```

#### 1.2.5. Combinando Ativa√ß√£o e Retropropaga√ß√£o - Algoritmo backpropagation


```python
# Algoritmo Backpropagation (calcula os gradientes para atualiza√ß√£o dos pesos)
# AL = Valor previsto no Forward
# Y = Valor real
def backward_propagation(AL, Y, caches):
    
    # Dicion√°rio para os gradientes
    grads = {}
    
    # Comprimento dos dados (que est√£o no cache)
    L = len(caches)
    
    # Extrai o comprimento para o valor de m
    m = AL.shape[1]
    
    # Ajusta o shape de Y
    Y = Y.reshape(AL.shape)
    
    # Calcula a derivada da previs√£o final da rede (feita ao final do Forward Propagation)
    dAL = -((Y / AL) - ((1 - Y) / (1 - AL)))
    
    # Captura o valor corrente do cache
    current_cache = caches[L - 1]
    
    # Gera a lista de gradiente para os dados, os pesos e o bias
    # Fazemos isso uma vez, pois estamos na parte final da rede, iniciando o caminho de volta
    grads["dA" + str(L - 1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation = "sigmoid")
    
    # Loop para calcular a derivada durante as ativa√ß√µes lineares com a relu
    for l in reversed(range(L - 1)):
        
        # Cache atual
        current_cache = caches[l]
        
        # Calcula as derivadas
        dA_prev, dW, db = linear_activation_backward(grads["dA" + str(l + 1)], current_cache, activation = "relu")
        
        # Alimenta os gradientes na lista, usando o √≠ndice respectivo
        grads["dA" + str(l)] = dA_prev
        grads["dW" + str(l + 1)] = dW
        grads["db" + str(l + 1)] = db
        
    return grads
```

#### 1.2.6. Gradientes e Atualiza√ß√£o dos Pesos


```python
# Fun√ß√£o de atualiza√ß√£o de pesos
def atualiza_pesos(parameters, grads, learning_rate):
    
    # Comprimento da estrutura de dados com os par√¢metros (pesos e bias)
    L = len(parameters)//2
    
    # Loop para atualiza√ß√£o dos pesos
    for l in range(L):
        
        # Atualiza√ß√£o dos pesos
        parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - (learning_rate * grads["dW" + str(l + 1)])
        
        # Atualiza√ß√£o do bias
        parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - (learning_rate * grads["db" + str(l + 1)])
    
    return parameters
```

### 1.3 Implementando a Rede Completa


```python
# Modelo completo da rede neural
def modeloNN(X, Y, dims_camada_entrada, learning_rate = 0.0075, num_iterations = 100):
    
    # Lista para receber o custo a cada √©poca de treinamento
    custos = []
    
    # Inicializa os par√¢metros
    parametros = inicializa_parametros(dims_camada_entrada)
    
    # Loop pelo n√∫mero de itera√ß√µes (√©pocas)
    for i in range(num_iterations):
        
        # Forward Propagation
        AL, caches = forward_propagation(X, parametros)
        
        # Calcula o custo
        custo = calcula_custo(AL, Y)
        
        # Backward Propagation
        # Nota: ao inv√©s de AL e Y, poder√≠amos passar somente o valor do custo
        # Estamos passando o valor de AL e Y para fique claro didaticamente o que est√° sendo feito
        gradientes = backward_propagation(AL, Y, caches)
        
        # Atualiza os pesos
        parametros = atualiza_pesos(parametros, gradientes, learning_rate)
        
        # Print do valor intermedi√°rio do custo
        # A redu√ß√£o do custo indica o aprendizado do modelo
        if i % 10 == 0:
            print("Custo Ap√≥s " + str(i) + " itera√ß√µes √© " + str(custo))
            custos.append(custo)
            
    return parametros, custos 
```


```python
# Fun√ß√£o para fazer as previs√µes
# N√£o precisamos do Backpropagation pois ao fazer previs√µes como o modelo treinado, 
# teremos os melhores valores de pesos (parametros)
def predict(X, parametros):
    AL, caches = forward_propagation(X, parametros)
    return AL
```

## 2.0. Usando Rede Naurais para Prever a Ocorr√™ncia de C√¢ncer


```python
# Import
import sklearn
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
```


```python
# Vers√µes dos pacotes usados neste jupyter notebook
%reload_ext watermark
%watermark -a "Rodolfo Terra | ci√™ncia de Dados" --iversions
```

    Author: Rodolfo Terra | ci√™ncia de Dados
    
    matplotlib: 3.3.4
    numpy     : 1.19.2
    pandas    : 1.2.3
    sklearn   : 0.0


‚Äã    

### 2.1 Carregando os dados


```python
# Carregando o objetov completo
temp = load_breast_cancer()
```


```python
# Tipo do Objetivo
type(temp)
```




    sklearn.utils.Bunch




```python
# Visualiza o objeto
temp
```




    {'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,
             1.189e-01],
            [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,
             8.902e-02],
            [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,
             8.758e-02],
            ...,
            [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,
             7.820e-02],
            [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,
             1.240e-01],
            [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,
             7.039e-02]]),
     'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
            0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
            1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
            1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
            1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
            0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
            1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
            0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
            1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
            1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
            0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
            0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
            1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
            1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
            1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
            1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
            1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
            1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),
     'frame': None,
     'target_names': array(['malignant', 'benign'], dtype='<U9'),
     'DESCR': '.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry\n        - fractal dimension ("coastline approximation" - 1)\n\n        The mean, standard error, and "worst" or largest (mean of the three\n        worst/largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n        10 is Radius SE, field 20 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree\nConstruction Via Linear Programming." Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: "Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171.',
     'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',
            'mean smoothness', 'mean compactness', 'mean concavity',
            'mean concave points', 'mean symmetry', 'mean fractal dimension',
            'radius error', 'texture error', 'perimeter error', 'area error',
            'smoothness error', 'compactness error', 'concavity error',
            'concave points error', 'symmetry error',
            'fractal dimension error', 'worst radius', 'worst texture',
            'worst perimeter', 'worst area', 'worst smoothness',
            'worst compactness', 'worst concavity', 'worst concave points',
            'worst symmetry', 'worst fractal dimension'], dtype='<U23'),
     'filename': 'E:\\anaconda\\lib\\site-packages\\sklearn\\datasets\\data\\breast_cancer.csv'}




```python
# Carregamos o dataset
dados = pd.DataFrame(columns = load_breast_cancer()['feature_names'], data = load_breast_cancer()['data'])
```


```python
# Shape
dados.shape
```




    (569, 30)




```python
# Visualiza os dados
dados.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>14.91</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>22.54</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
  </tbody>
</table>
<p>5 rows √ó 30 columns</p>
</div>




```python
#Verifica se temos valores ausentes
dados.isnull().any()
```




    mean radius                False
    mean texture               False
    mean perimeter             False
    mean area                  False
    mean smoothness            False
    mean compactness           False
    mean concavity             False
    mean concave points        False
    mean symmetry              False
    mean fractal dimension     False
    radius error               False
    texture error              False
    perimeter error            False
    area error                 False
    smoothness error           False
    compactness error          False
    concavity error            False
    concave points error       False
    symmetry error             False
    fractal dimension error    False
    worst radius               False
    worst texture              False
    worst perimeter            False
    worst area                 False
    worst smoothness           False
    worst compactness          False
    worst concavity            False
    worst concave points       False
    worst symmetry             False
    worst fractal dimension    False
    dtype: bool




```python
# Separa a vari√°vel target
target = load_breast_cancer()['target']
```


```python
type(target)
```




    numpy.ndarray




```python
# Visualiza a vari√°vel
target
```




    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
           0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
           1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
           1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
           1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
           0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
           1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
           0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
           1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
           1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
           0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
           0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
           1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
           1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
           1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
           1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
           1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])




```python
# Total de registro por classe - C√¢ncer Benigno
np.count_nonzero(target == 1)

```




    357




```python
# Total de registrtos por class - Cancer Mal√≠gno
np.count_nonzero(target == 0)
```




    212




```python
# Vamos extrair os labels

# Dicion√°rio para labels
labels = {}

# Nomes das classes das vari√°vel taregt
target_names = load_breast_cancer()['target_names']

#Mapeamento
for i in range(len(target_names)):
    labels.update({i:target_names[i]})
```


```python
# Visualiza os lables
labels
```




    {0: 'malignant', 1: 'benign'}




```python
# Agora preparamos as vari√°veis preditora em x
X = np.array(dados)
```


```python
# Visualiza os dados de entrada
X
```




    array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,
            1.189e-01],
           [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,
            8.902e-02],
           [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,
            8.758e-02],
           ...,
           [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,
            7.820e-02],
           [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,
            1.240e-01],
           [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,
            7.039e-02]])




```python
# Dividimos os dados de entrada e sa√≠da em treino e teste
X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, target, test_size = 0.15, shuffle = True)
```


```python
# Shape dos dados de treino
print(X_treino.shape)
print(Y_treino.shape)
```

    (483, 30)
    (483,)



```python
# Shape dos dados de teste
print(X_teste.shape)
print(Y_teste.shape)
```

    (86, 30)
    (86,)



```python
# Ajuste o shape dos dados de entrada
X_treino = X_treino.T
X_teste = X_teste.T
```


```python
print(X_treino.shape)
print(X_teste.shape)
```

    (30, 483)
    (30, 86)



```python
# Precisamos ajustar tamb√©m os dados de sa√≠da
Y_treino = Y_treino.reshape(1, len(Y_treino))
Y_teste = Y_teste.reshape(1, len(Y_teste))
```


```python
print(Y_treino.shape)
print(Y_teste.shape)
```

    (1, 483)
    (1, 86)



```python
# Vari√°vel com as dimens√µes de entrada para o n√∫mero de neur√¥nios
dims_camada_entrada = [X_treino.shape[0], 50, 20, 5, 1]
```


```python
dims_camada_entrada
```




    [30, 50, 20, 5, 1]




```python
# Treinamento do modelo

print("\nIniciando o Treinamento.\n")

parametros, custo = modeloNN(X = X_treino, 
                             Y = Y_treino, 
                             dims_camada_entrada = dims_camada_entrada, 
                             num_iterations = 3000, 
                             learning_rate = 0.0075)

print("\nTreinamento Conclu√≠do.\n")
```


    Iniciando o Treinamento.
    
    Custo Ap√≥s 0 itera√ß√µes √© 0.6931354827911539
    Custo Ap√≥s 10 itera√ß√µes √© 0.6920510189719492
    Custo Ap√≥s 20 itera√ß√µes √© 0.691006675432112
    Custo Ap√≥s 30 itera√ß√µes √© 0.6900005871078156
    Custo Ap√≥s 40 itera√ß√µes √© 0.6890316826286887
    Custo Ap√≥s 50 itera√ß√µes √© 0.6880989818052369
    Custo Ap√≥s 60 itera√ß√µes √© 0.6872009409838412
    Custo Ap√≥s 70 itera√ß√µes √© 0.6863359754322764
    Custo Ap√≥s 80 itera√ß√µes √© 0.6855027387456888
    Custo Ap√≥s 90 itera√ß√µes √© 0.684700035843715
    Custo Ap√≥s 100 itera√ß√µes √© 0.6839267197912282
    Custo Ap√≥s 110 itera√ß√µes √© 0.6831816861022018
    Custo Ap√≥s 120 itera√ß√µes √© 0.6824638758175816
    Custo Ap√≥s 130 itera√ß√µes √© 0.6817722647923482
    Custo Ap√≥s 140 itera√ß√µes √© 0.6811058689690097
    Custo Ap√≥s 150 itera√ß√µes √© 0.6804637406537173
    Custo Ap√≥s 160 itera√ß√µes √© 0.6798449708351277
    Custo Ap√≥s 170 itera√ß√µes √© 0.6792486749248884
    Custo Ap√≥s 180 itera√ß√µes √© 0.6786739963428489
    Custo Ap√≥s 190 itera√ß√µes √© 0.6781201110931454
    Custo Ap√≥s 200 itera√ß√µes √© 0.6775862268500347
    Custo Ap√≥s 210 itera√ß√µes √© 0.6770715788429543
    Custo Ap√≥s 220 itera√ß√µes √© 0.676575440396268
    Custo Ap√≥s 230 itera√ß√µes √© 0.6760970950314643
    Custo Ap√≥s 240 itera√ß√µes √© 0.6756358438304763
    Custo Ap√≥s 250 itera√ß√µes √© 0.6751910215714026
    Custo Ap√≥s 260 itera√ß√µes √© 0.6747619826481853
    Custo Ap√≥s 270 itera√ß√µes √© 0.6743480908496903
    Custo Ap√≥s 280 itera√ß√µes √© 0.6739487105885198
    Custo Ap√≥s 290 itera√ß√µes √© 0.673563265261196
    Custo Ap√≥s 300 itera√ß√µes √© 0.6731911843168401
    Custo Ap√≥s 310 itera√ß√µes √© 0.672831898809141
    Custo Ap√≥s 320 itera√ß√µes √© 0.6724848559335725
    Custo Ap√≥s 330 itera√ß√µes √© 0.6721495073356644
    Custo Ap√≥s 340 itera√ß√µes √© 0.6718253187820605
    Custo Ap√≥s 350 itera√ß√µes √© 0.6715117498064969
    Custo Ap√≥s 360 itera√ß√µes √© 0.6712082678613013
    Custo Ap√≥s 370 itera√ß√µes √© 0.6709143391647661
    Custo Ap√≥s 380 itera√ß√µes √© 0.670629419795754
    Custo Ap√≥s 390 itera√ß√µes √© 0.6703529546977492
    Custo Ap√≥s 400 itera√ß√µes √© 0.6700843655475854
    Custo Ap√≥s 410 itera√ß√µes √© 0.669823057381083
    Custo Ap√≥s 420 itera√ß√µes √© 0.6695683911494233
    Custo Ap√≥s 430 itera√ß√µes √© 0.669319710048415
    Custo Ap√≥s 440 itera√ß√µes √© 0.6690762877130361
    Custo Ap√≥s 450 itera√ß√µes √© 0.6688373302856894
    Custo Ap√≥s 460 itera√ß√µes √© 0.6686019733826961
    Custo Ap√≥s 470 itera√ß√µes √© 0.668369195243311
    Custo Ap√≥s 480 itera√ß√µes √© 0.6681377930402528
    Custo Ap√≥s 490 itera√ß√µes √© 0.6679063665796368
    Custo Ap√≥s 500 itera√ß√µes √© 0.6676731728034567
    Custo Ap√≥s 510 itera√ß√µes √© 0.667436089996904
    Custo Ap√≥s 520 itera√ß√µes √© 0.6671923936419432
    Custo Ap√≥s 530 itera√ß√µes √© 0.6669386008683635
    Custo Ap√≥s 540 itera√ß√µes √© 0.6666701566248439
    Custo Ap√≥s 550 itera√ß√µes √© 0.6663809784986686
    Custo Ap√≥s 560 itera√ß√µes √© 0.6660626852506472
    Custo Ap√≥s 570 itera√ß√µes √© 0.6657035761786466
    Custo Ap√≥s 580 itera√ß√µes √© 0.6652868556230841
    Custo Ap√≥s 590 itera√ß√µes √© 0.6647876755405753
    Custo Ap√≥s 600 itera√ß√µes √© 0.664168228608324
    Custo Ap√≥s 610 itera√ß√µes √© 0.6633684658296017
    Custo Ap√≥s 620 itera√ß√µes √© 0.6622852483896856
    Custo Ap√≥s 630 itera√ß√µes √© 0.6607488318586557
    Custo Ap√≥s 640 itera√ß√µes √© 0.6584878437033544
    Custo Ap√≥s 650 itera√ß√µes √© 0.6549627460396508
    Custo Ap√≥s 660 itera√ß√µes √© 0.6492433357260329
    Custo Ap√≥s 670 itera√ß√µes √© 0.6400864593104626
    Custo Ap√≥s 680 itera√ß√µes √© 0.6276850189677998
    Custo Ap√≥s 690 itera√ß√µes √© 0.6170853356772148
    Custo Ap√≥s 700 itera√ß√µes √© 0.6118568875437549
    Custo Ap√≥s 710 itera√ß√µes √© 0.6088694948898659
    Custo Ap√≥s 720 itera√ß√µes √© 0.6061898414701167
    Custo Ap√≥s 730 itera√ß√µes √© 0.6035119898381162
    Custo Ap√≥s 740 itera√ß√µes √© 0.6008010968470019
    Custo Ap√≥s 750 itera√ß√µes √© 0.5979959560105433
    Custo Ap√≥s 760 itera√ß√µes √© 0.5951961135369327
    Custo Ap√≥s 770 itera√ß√µes √© 0.5923416564229492
    Custo Ap√≥s 780 itera√ß√µes √© 0.5894171037449164
    Custo Ap√≥s 790 itera√ß√µes √© 0.5864063304116273
    Custo Ap√≥s 800 itera√ß√µes √© 0.5832891935956854
    Custo Ap√≥s 810 itera√ß√µes √© 0.580039205344321
    Custo Ap√≥s 820 itera√ß√µes √© 0.5766214997611934
    Custo Ap√≥s 830 itera√ß√µes √© 0.5729896756084699
    Custo Ap√≥s 840 itera√ß√µes √© 0.5690806449678082
    Custo Ap√≥s 850 itera√ß√µes √© 0.5648072065371783
    Custo Ap√≥s 860 itera√ß√µes √© 0.5600468813647359
    Custo Ap√≥s 870 itera√ß√µes √© 0.5545812288178429
    Custo Ap√≥s 880 itera√ß√µes √© 0.548251562653865
    Custo Ap√≥s 890 itera√ß√µes √© 0.5407154555695491
    Custo Ap√≥s 900 itera√ß√µes √© 0.5315257906198648
    Custo Ap√≥s 910 itera√ß√µes √© 0.5200605451807121
    Custo Ap√≥s 920 itera√ß√µes √© 0.5056317781100451
    Custo Ap√≥s 930 itera√ß√µes √© 0.48787907365711797
    Custo Ap√≥s 940 itera√ß√µes √© 0.4675435469161462
    Custo Ap√≥s 950 itera√ß√µes √© 0.4478636063648477
    Custo Ap√≥s 960 itera√ß√µes √© 0.48383219580818104
    Custo Ap√≥s 970 itera√ß√µes √© 0.4749807833227217
    Custo Ap√≥s 980 itera√ß√µes √© 0.4645590829757372
    Custo Ap√≥s 990 itera√ß√µes √© 0.4546484330130221
    Custo Ap√≥s 1000 itera√ß√µes √© 0.44372852225665377
    Custo Ap√≥s 1010 itera√ß√µes √© 0.4353041441552156
    Custo Ap√≥s 1020 itera√ß√µes √© 0.42851625299551827
    Custo Ap√≥s 1030 itera√ß√µes √© 0.4200871754974273
    Custo Ap√≥s 1040 itera√ß√µes √© 0.41333041135920895
    Custo Ap√≥s 1050 itera√ß√µes √© 0.40756497209103465
    Custo Ap√≥s 1060 itera√ß√µes √© 0.4015559472227128
    Custo Ap√≥s 1070 itera√ß√µes √© 0.39719653365511176
    Custo Ap√≥s 1080 itera√ß√µes √© 0.3906812049538403
    Custo Ap√≥s 1090 itera√ß√µes √© 0.3886395727310075
    Custo Ap√≥s 1100 itera√ß√µes √© 0.3863821559609492
    Custo Ap√≥s 1110 itera√ß√µes √© 0.38104103160610187
    Custo Ap√≥s 1120 itera√ß√µes √© 0.37749903303222904
    Custo Ap√≥s 1130 itera√ß√µes √© 0.37540935525288366
    Custo Ap√≥s 1140 itera√ß√µes √© 0.37275157298313816
    Custo Ap√≥s 1150 itera√ß√µes √© 0.3692658124782557
    Custo Ap√≥s 1160 itera√ß√µes √© 0.36764868279637597
    Custo Ap√≥s 1170 itera√ß√µes √© 0.3653497369782557
    Custo Ap√≥s 1180 itera√ß√µes √© 0.36065393626258513
    Custo Ap√≥s 1190 itera√ß√µes √© 0.3582679322684965
    Custo Ap√≥s 1200 itera√ß√µes √© 0.35580147795448386
    Custo Ap√≥s 1210 itera√ß√µes √© 0.35388803878718567
    Custo Ap√≥s 1220 itera√ß√µes √© 0.3536850706975638
    Custo Ap√≥s 1230 itera√ß√µes √© 0.35177659681184387
    Custo Ap√≥s 1240 itera√ß√µes √© 0.34985947378719806
    Custo Ap√≥s 1250 itera√ß√µes √© 0.34758128922191056
    Custo Ap√≥s 1260 itera√ß√µes √© 0.3431840812317904
    Custo Ap√≥s 1270 itera√ß√µes √© 0.3408483686984917
    Custo Ap√≥s 1280 itera√ß√µes √© 0.34161383932116424
    Custo Ap√≥s 1290 itera√ß√µes √© 0.33860307269098966
    Custo Ap√≥s 1300 itera√ß√µes √© 0.33743326994065304
    Custo Ap√≥s 1310 itera√ß√µes √© 0.3356035251367847
    Custo Ap√≥s 1320 itera√ß√µes √© 0.33401415544994506
    Custo Ap√≥s 1330 itera√ß√µes √© 0.3332543519999735
    Custo Ap√≥s 1340 itera√ß√µes √© 0.33211239535647236
    Custo Ap√≥s 1350 itera√ß√µes √© 0.33207690094967973
    Custo Ap√≥s 1360 itera√ß√µes √© 0.33079456507306615
    Custo Ap√≥s 1370 itera√ß√µes √© 0.32823102847703256
    Custo Ap√≥s 1380 itera√ß√µes √© 0.3281361970070891
    Custo Ap√≥s 1390 itera√ß√µes √© 0.3254599797770451
    Custo Ap√≥s 1400 itera√ß√µes √© 0.3233665379412725
    Custo Ap√≥s 1410 itera√ß√µes √© 0.3223414004156845
    Custo Ap√≥s 1420 itera√ß√µes √© 0.3205577732074268
    Custo Ap√≥s 1430 itera√ß√µes √© 0.31916497989875287
    Custo Ap√≥s 1440 itera√ß√µes √© 0.31872642957069436
    Custo Ap√≥s 1450 itera√ß√µes √© 0.31795280056972597
    Custo Ap√≥s 1460 itera√ß√µes √© 0.31635575991988857
    Custo Ap√≥s 1470 itera√ß√µes √© 0.3149592396139368
    Custo Ap√≥s 1480 itera√ß√µes √© 0.31472530303348595
    Custo Ap√≥s 1490 itera√ß√µes √© 0.31509298189183677
    Custo Ap√≥s 1500 itera√ß√µes √© 0.3115717537400463
    Custo Ap√≥s 1510 itera√ß√µes √© 0.31359955149500934
    Custo Ap√≥s 1520 itera√ß√µes √© 0.3092826871600542
    Custo Ap√≥s 1530 itera√ß√µes √© 0.3118336406434141
    Custo Ap√≥s 1540 itera√ß√µes √© 0.30709530530438434
    Custo Ap√≥s 1550 itera√ß√µes √© 0.3082550676207331
    Custo Ap√≥s 1560 itera√ß√µes √© 0.306414716892454
    Custo Ap√≥s 1570 itera√ß√µes √© 0.30738038079320906
    Custo Ap√≥s 1580 itera√ß√µes √© 0.30521224078764425
    Custo Ap√≥s 1590 itera√ß√µes √© 0.3049474583355111
    Custo Ap√≥s 1600 itera√ß√µes √© 0.3026583401878312
    Custo Ap√≥s 1610 itera√ß√µes √© 0.30463669511159774
    Custo Ap√≥s 1620 itera√ß√µes √© 0.30226143020745244
    Custo Ap√≥s 1630 itera√ß√µes √© 0.3017362091043683
    Custo Ap√≥s 1640 itera√ß√µes √© 0.3017574542546733
    Custo Ap√≥s 1650 itera√ß√µes √© 0.3002416197318212
    Custo Ap√≥s 1660 itera√ß√µes √© 0.2953460194508413
    Custo Ap√≥s 1670 itera√ß√µes √© 0.2900817843618759
    Custo Ap√≥s 1680 itera√ß√µes √© 0.28789682403594613
    Custo Ap√≥s 1690 itera√ß√µes √© 0.28711220060463816
    Custo Ap√≥s 1700 itera√ß√µes √© 0.2869651992642646
    Custo Ap√≥s 1710 itera√ß√µes √© 0.2859384443188689
    Custo Ap√≥s 1720 itera√ß√µes √© 0.285506966339705
    Custo Ap√≥s 1730 itera√ß√µes √© 0.2853122823455955
    Custo Ap√≥s 1740 itera√ß√µes √© 0.2842682696635921
    Custo Ap√≥s 1750 itera√ß√µes √© 0.28499655120382006
    Custo Ap√≥s 1760 itera√ß√µes √© 0.28309923409824306
    Custo Ap√≥s 1770 itera√ß√µes √© 0.2824325553155139
    Custo Ap√≥s 1780 itera√ß√µes √© 0.2821065266012859
    Custo Ap√≥s 1790 itera√ß√µes √© 0.2808897073445936
    Custo Ap√≥s 1800 itera√ß√µes √© 0.2835889515687025
    Custo Ap√≥s 1810 itera√ß√µes √© 0.29292810405028163
    Custo Ap√≥s 1820 itera√ß√µes √© 0.2896877820985566
    Custo Ap√≥s 1830 itera√ß√µes √© 0.28833697304458356
    Custo Ap√≥s 1840 itera√ß√µes √© 0.2848410816894288
    Custo Ap√≥s 1850 itera√ß√µes √© 0.278579914662654
    Custo Ap√≥s 1860 itera√ß√µes √© 0.2754053340697972
    Custo Ap√≥s 1870 itera√ß√µes √© 0.27611139219638065
    Custo Ap√≥s 1880 itera√ß√µes √© 0.27519156409300866
    Custo Ap√≥s 1890 itera√ß√µes √© 0.27479831014282774
    Custo Ap√≥s 1900 itera√ß√µes √© 0.27398744604966013
    Custo Ap√≥s 1910 itera√ß√µes √© 0.27395340442774146
    Custo Ap√≥s 1920 itera√ß√µes √© 0.2728521519066461
    Custo Ap√≥s 1930 itera√ß√µes √© 0.2726147349862487
    Custo Ap√≥s 1940 itera√ß√µes √© 0.2720434453731387
    Custo Ap√≥s 1950 itera√ß√µes √© 0.27078419152251626
    Custo Ap√≥s 1960 itera√ß√µes √© 0.27143930259363624
    Custo Ap√≥s 1970 itera√ß√µes √© 0.2759605835064127
    Custo Ap√≥s 1980 itera√ß√µes √© 0.28037108663104576
    Custo Ap√≥s 1990 itera√ß√µes √© 0.2784812900269843
    Custo Ap√≥s 2000 itera√ß√µes √© 0.27652988874789275
    Custo Ap√≥s 2010 itera√ß√µes √© 0.2715704221621722
    Custo Ap√≥s 2020 itera√ß√µes √© 0.2709978272162304
    Custo Ap√≥s 2030 itera√ß√µes √© 0.271094279234915
    Custo Ap√≥s 2040 itera√ß√µes √© 0.2698874001836786
    Custo Ap√≥s 2050 itera√ß√µes √© 0.2697405095232057
    Custo Ap√≥s 2060 itera√ß√µes √© 0.2689097701354004
    Custo Ap√≥s 2070 itera√ß√µes √© 0.2687631783206506
    Custo Ap√≥s 2080 itera√ß√µes √© 0.2685338507619344
    Custo Ap√≥s 2090 itera√ß√µes √© 0.2676585287927146
    Custo Ap√≥s 2100 itera√ß√µes √© 0.2658432909149376
    Custo Ap√≥s 2110 itera√ß√µes √© 0.2645160807472383
    Custo Ap√≥s 2120 itera√ß√µes √© 0.26244825366112523
    Custo Ap√≥s 2130 itera√ß√µes √© 0.2623245505140088
    Custo Ap√≥s 2140 itera√ß√µes √© 0.26222760350181523
    Custo Ap√≥s 2150 itera√ß√µes √© 0.2623082041553852
    Custo Ap√≥s 2160 itera√ß√µes √© 0.2619062242395529
    Custo Ap√≥s 2170 itera√ß√µes √© 0.2627164630493991
    Custo Ap√≥s 2180 itera√ß√µes √© 0.26235066361287307
    Custo Ap√≥s 2190 itera√ß√µes √© 0.26175632940609483
    Custo Ap√≥s 2200 itera√ß√µes √© 0.26100691636685264
    Custo Ap√≥s 2210 itera√ß√µes √© 0.26061579152493436
    Custo Ap√≥s 2220 itera√ß√µes √© 0.2596239066560759
    Custo Ap√≥s 2230 itera√ß√µes √© 0.2589493012440935
    Custo Ap√≥s 2240 itera√ß√µes √© 0.25973676659783274
    Custo Ap√≥s 2250 itera√ß√µes √© 0.25908732945410917
    Custo Ap√≥s 2260 itera√ß√µes √© 0.2592245762338003
    Custo Ap√≥s 2270 itera√ß√µes √© 0.25877442613524376
    Custo Ap√≥s 2280 itera√ß√µes √© 0.2579188511550558
    Custo Ap√≥s 2290 itera√ß√µes √© 0.2584274483319684
    Custo Ap√≥s 2300 itera√ß√µes √© 0.2574587659288542
    Custo Ap√≥s 2310 itera√ß√µes √© 0.25804824175631175
    Custo Ap√≥s 2320 itera√ß√µes √© 0.2563444889848643
    Custo Ap√≥s 2330 itera√ß√µes √© 0.2561864422715731
    Custo Ap√≥s 2340 itera√ß√µes √© 0.25635901756391033
    Custo Ap√≥s 2350 itera√ß√µes √© 0.25592462634382623
    Custo Ap√≥s 2360 itera√ß√µes √© 0.25506150284038
    Custo Ap√≥s 2370 itera√ß√µes √© 0.25366444065353605
    Custo Ap√≥s 2380 itera√ß√µes √© 0.2546946933163173
    Custo Ap√≥s 2390 itera√ß√µes √© 0.25385895861458463
    Custo Ap√≥s 2400 itera√ß√µes √© 0.25351553415415595
    Custo Ap√≥s 2410 itera√ß√µes √© 0.25351787943629167
    Custo Ap√≥s 2420 itera√ß√µes √© 0.25287819940823675
    Custo Ap√≥s 2430 itera√ß√µes √© 0.2509861432752116
    Custo Ap√≥s 2440 itera√ß√µes √© 0.25156909210873707
    Custo Ap√≥s 2450 itera√ß√µes √© 0.24941821681042067
    Custo Ap√≥s 2460 itera√ß√µes √© 0.25247157890428007
    Custo Ap√≥s 2470 itera√ß√µes √© 0.2518079811759134
    Custo Ap√≥s 2480 itera√ß√µes √© 0.25070073445456764
    Custo Ap√≥s 2490 itera√ß√µes √© 0.25104965198794854
    Custo Ap√≥s 2500 itera√ß√µes √© 0.2482824666586698
    Custo Ap√≥s 2510 itera√ß√µes √© 0.25112229069648073
    Custo Ap√≥s 2520 itera√ß√µes √© 0.24907980922646247
    Custo Ap√≥s 2530 itera√ß√µes √© 0.2477061141143648
    Custo Ap√≥s 2540 itera√ß√µes √© 0.24739257346759017
    Custo Ap√≥s 2550 itera√ß√µes √© 0.24639473148195173
    Custo Ap√≥s 2560 itera√ß√µes √© 0.2469249195285623
    Custo Ap√≥s 2570 itera√ß√µes √© 0.24744701598841085
    Custo Ap√≥s 2580 itera√ß√µes √© 0.2468786359281917
    Custo Ap√≥s 2590 itera√ß√µes √© 0.24762889935607021
    Custo Ap√≥s 2600 itera√ß√µes √© 0.24695642602301876
    Custo Ap√≥s 2610 itera√ß√µes √© 0.24608397119720832
    Custo Ap√≥s 2620 itera√ß√µes √© 0.2468648996935379
    Custo Ap√≥s 2630 itera√ß√µes √© 0.24389008057852257
    Custo Ap√≥s 2640 itera√ß√µes √© 0.24484846229185056
    Custo Ap√≥s 2650 itera√ß√µes √© 0.2438607350649672
    Custo Ap√≥s 2660 itera√ß√µes √© 0.24483624173100807
    Custo Ap√≥s 2670 itera√ß√µes √© 0.24327072267459207
    Custo Ap√≥s 2680 itera√ß√µes √© 0.2431097737217176
    Custo Ap√≥s 2690 itera√ß√µes √© 0.24391130983874929
    Custo Ap√≥s 2700 itera√ß√µes √© 0.24162714753901887
    Custo Ap√≥s 2710 itera√ß√µes √© 0.24412081135810546
    Custo Ap√≥s 2720 itera√ß√µes √© 0.24368174821692948
    Custo Ap√≥s 2730 itera√ß√µes √© 0.24291720696529326
    Custo Ap√≥s 2740 itera√ß√µes √© 0.23995695161715225
    Custo Ap√≥s 2750 itera√ß√µes √© 0.24210291045963786
    Custo Ap√≥s 2760 itera√ß√µes √© 0.23980092619010304
    Custo Ap√≥s 2770 itera√ß√µes √© 0.24025681764598336
    Custo Ap√≥s 2780 itera√ß√µes √© 0.241905782438928
    Custo Ap√≥s 2790 itera√ß√µes √© 0.23892841378201982
    Custo Ap√≥s 2800 itera√ß√µes √© 0.24039095675665495
    Custo Ap√≥s 2810 itera√ß√µes √© 0.23953326262927752
    Custo Ap√≥s 2820 itera√ß√µes √© 0.23854003681720137
    Custo Ap√≥s 2830 itera√ß√µes √© 0.24035765180039503
    Custo Ap√≥s 2840 itera√ß√µes √© 0.23938815925499868
    Custo Ap√≥s 2850 itera√ß√µes √© 0.2362577610490712
    Custo Ap√≥s 2860 itera√ß√µes √© 0.23853005578934694
    Custo Ap√≥s 2870 itera√ß√µes √© 0.23736034523610233
    Custo Ap√≥s 2880 itera√ß√µes √© 0.23727333512226575
    Custo Ap√≥s 2890 itera√ß√µes √© 0.23632454433366096
    Custo Ap√≥s 2900 itera√ß√µes √© 0.23526470846598052
    Custo Ap√≥s 2910 itera√ß√µes √© 0.23657173332213094
    Custo Ap√≥s 2920 itera√ß√µes √© 0.23647788977633055
    Custo Ap√≥s 2930 itera√ß√µes √© 0.23576064490677767
    Custo Ap√≥s 2940 itera√ß√µes √© 0.23528132319678388
    Custo Ap√≥s 2950 itera√ß√µes √© 0.23595867270233317
    Custo Ap√≥s 2960 itera√ß√µes √© 0.2347209073979776
    Custo Ap√≥s 2970 itera√ß√µes √© 0.23486409522917304
    Custo Ap√≥s 2980 itera√ß√µes √© 0.2348239400874354
    Custo Ap√≥s 2990 itera√ß√µes √© 0.23568367578082658
    
    Treinamento Conclu√≠do.


‚Äã    


```python
# Plot do erro durante o treinamento
plt.plot(custo)
```




    [<matplotlib.lines.Line2D at 0x21342f14220>]




![png](imagens/output_77_1.png)
    



```python
# Previs√µes com os dados de treino
y_pred_treino = predict(X_treino, parametros)
```


```python
# Visualiza as previs√µes
y_pred_treino
```




    array([[8.77196133e-01, 7.41472924e-01, 8.20416721e-01, 8.68722475e-01,
            8.78556036e-01, 4.04073209e-09, 7.29018985e-02, 8.52131379e-01,
            8.70576445e-01, 8.78455793e-01, 8.78761511e-01, 8.57622405e-10,
            9.21189348e-08, 8.76671957e-01, 7.69496087e-01, 8.78216187e-01,
            4.56993610e-07, 8.77354039e-01, 8.76457673e-01, 8.49480334e-01,
            7.75628374e-03, 5.51302163e-01, 3.46637675e-12, 1.45319248e-08,
            8.78024204e-01, 8.76942248e-01, 7.41478822e-01, 8.77159477e-01,
            8.73723356e-01, 3.01569223e-08, 8.92129808e-07, 1.60117359e-06,
            8.56617206e-01, 8.78474526e-01, 5.24161907e-01, 1.57711501e-05,
            6.20519426e-04, 8.76216725e-01, 1.76591702e-01, 8.39496710e-01,
            8.68658927e-01, 8.78353368e-01, 8.77477964e-01, 8.62520765e-01,
            3.20608785e-09, 8.77056705e-01, 8.39286735e-01, 8.74755090e-01,
            8.00109669e-01, 7.97238762e-01, 7.58013693e-01, 8.77910692e-01,
            7.50175639e-01, 3.69612772e-08, 8.78787607e-01, 8.78236756e-01,
            8.11537184e-01, 8.30197617e-01, 4.18120691e-01, 1.93712559e-01,
            1.61447706e-01, 1.09351859e-05, 1.19531241e-05, 8.78511393e-01,
            8.77532394e-01, 8.47188392e-01, 8.73288194e-01, 4.85265757e-01,
            3.57907843e-02, 6.23963048e-03, 7.20686971e-01, 4.05488331e-02,
            3.61140228e-01, 4.05773172e-02, 3.33043988e-03, 7.97209407e-01,
            4.27131521e-01, 8.78449042e-01, 6.86809269e-01, 1.16149586e-02,
            8.78574405e-01, 8.56679990e-01, 8.52727065e-01, 4.51067801e-04,
            8.75077347e-01, 8.28353328e-01, 7.74118982e-01, 7.43589678e-01,
            1.16344879e-01, 3.83157591e-04, 8.40290426e-01, 8.78470052e-01,
            8.77736021e-01, 8.78006565e-01, 4.27211595e-01, 7.63897371e-06,
            8.46263818e-01, 8.08256421e-01, 5.74532910e-01, 8.78635278e-01,
            8.72632971e-05, 8.77610206e-01, 2.20767329e-04, 8.68363679e-01,
            8.78472506e-01, 8.78505146e-01, 8.78612928e-01, 8.74866409e-01,
            8.13378840e-01, 8.77107783e-01, 2.69209600e-06, 2.60771549e-05,
            8.71871057e-01, 8.78426518e-01, 7.67209789e-01, 2.34500618e-17,
            4.17462060e-01, 7.10162368e-02, 1.93122918e-17, 1.12911102e-07,
            2.50758082e-01, 7.52198536e-01, 1.26822383e-02, 6.82646094e-01,
            3.77119476e-05, 8.77716576e-01, 3.08025298e-10, 7.15111920e-01,
            8.78037289e-01, 8.33603789e-01, 1.88776023e-03, 8.77942703e-01,
            8.65099932e-01, 2.59277118e-01, 2.12538067e-01, 8.78435277e-01,
            4.34640308e-02, 4.68600865e-02, 8.78192030e-01, 7.19544765e-01,
            6.93530691e-01, 8.64277645e-01, 8.78364211e-01, 5.73807022e-01,
            8.77980949e-01, 1.77085781e-03, 8.66796805e-01, 3.66465111e-13,
            8.77666924e-01, 7.88028668e-01, 4.69087095e-02, 8.64139582e-01,
            8.50983318e-01, 1.92639560e-05, 2.14971703e-03, 8.18318003e-01,
            6.00523818e-01, 1.51028794e-01, 8.78734723e-01, 2.02483043e-11,
            1.40939766e-01, 8.77946166e-01, 8.77613520e-01, 8.78250423e-01,
            8.77770509e-01, 1.13514890e-03, 4.94427346e-01, 8.78415974e-01,
            7.37259726e-02, 8.75076953e-01, 8.64552282e-01, 8.25776364e-01,
            8.78515781e-01, 7.52146089e-01, 6.22318696e-08, 8.40146808e-01,
            8.84726018e-10, 3.51410553e-02, 7.87621301e-01, 7.87083860e-01,
            6.20683519e-01, 8.36625517e-01, 8.77575806e-01, 6.22782509e-10,
            7.14457659e-01, 8.77618824e-01, 8.77950430e-01, 8.47332609e-01,
            8.61280676e-01, 8.73792407e-01, 8.64050019e-01, 4.80952153e-01,
            8.04975482e-01, 8.78510297e-01, 8.71264491e-01, 8.18273532e-01,
            7.34244495e-01, 8.52098499e-01, 7.94469740e-01, 3.19547286e-06,
            7.58157448e-01, 8.56524897e-01, 8.74401651e-01, 1.98090340e-04,
            1.00960426e-04, 6.14180757e-01, 8.71569392e-01, 8.78213449e-01,
            1.76948315e-05, 7.81120060e-01, 2.52890794e-13, 8.78525632e-01,
            4.26431336e-02, 5.65273961e-01, 7.14343758e-01, 8.29378094e-01,
            8.56935169e-01, 4.18933359e-04, 6.31810700e-01, 7.33367120e-05,
            8.78542779e-01, 2.24586194e-01, 7.62041731e-01, 1.98397308e-01,
            8.66559343e-04, 8.16201611e-01, 8.75465086e-01, 8.66209307e-01,
            7.68402243e-01, 6.10394303e-03, 8.41193613e-01, 8.77762577e-01,
            8.77042876e-01, 1.97262377e-03, 8.77529941e-01, 1.93557259e-04,
            1.56594592e-02, 4.36044398e-06, 8.75334683e-01, 8.78039585e-01,
            8.78117307e-01, 1.45260850e-03, 8.73896535e-01, 1.25173344e-06,
            8.76682138e-01, 6.97039535e-01, 8.73846104e-01, 8.48351112e-01,
            8.78067554e-01, 8.78080537e-01, 8.78517149e-01, 8.07066523e-01,
            1.89941244e-06, 8.77529602e-01, 8.74987386e-01, 8.54380858e-01,
            1.04288600e-08, 8.77395646e-01, 6.72586132e-01, 8.78037859e-01,
            8.78333100e-01, 2.94964205e-06, 2.28607394e-05, 6.69443304e-02,
            3.68077109e-11, 1.15909218e-03, 3.35914079e-09, 4.93304967e-01,
            6.52645418e-01, 4.86995606e-07, 5.96477423e-06, 8.78713612e-01,
            3.97936372e-14, 8.77838755e-01, 6.63420721e-01, 1.17326291e-04,
            3.41550115e-01, 8.78554903e-01, 8.78534244e-01, 8.34398381e-01,
            8.23275446e-01, 8.08548657e-01, 9.78751174e-05, 3.00391968e-03,
            8.74561798e-01, 8.78625093e-01, 7.56913583e-01, 8.77837092e-01,
            6.22043926e-01, 2.09726360e-02, 3.47545039e-29, 8.38910717e-01,
            8.78035538e-01, 8.77327587e-01, 8.72296226e-01, 3.68135999e-01,
            7.24476403e-01, 1.37277754e-02, 1.08209935e-06, 3.18495327e-06,
            4.48232623e-05, 8.70640985e-01, 7.98503430e-01, 6.78402370e-01,
            8.65127690e-01, 8.78165747e-01, 7.72778952e-01, 8.78383277e-01,
            2.56663957e-15, 1.61311432e-01, 8.72864927e-01, 8.45856633e-01,
            8.49176195e-01, 5.83169797e-01, 8.77230100e-01, 1.74830529e-13,
            8.12202346e-01, 7.95595836e-01, 8.78597652e-01, 8.77855826e-01,
            7.72122834e-01, 6.78441413e-01, 8.77679578e-01, 8.78436172e-01,
            1.39185655e-04, 7.50345148e-01, 8.78418146e-01, 8.24969171e-01,
            8.53461694e-01, 8.78669943e-01, 8.78071757e-01, 6.11486049e-01,
            8.37306775e-01, 1.33914178e-03, 6.31358636e-02, 6.26134847e-01,
            5.29583951e-04, 8.29979563e-01, 8.00940152e-01, 8.70357376e-01,
            6.28785117e-01, 8.66529205e-01, 8.77509627e-01, 7.37882174e-01,
            6.44500180e-07, 1.35367923e-04, 8.28133617e-01, 8.76903411e-01,
            3.48008246e-05, 8.78858342e-01, 7.22611140e-01, 6.95609815e-01,
            2.37915272e-04, 7.26725042e-29, 2.00461315e-01, 1.61258563e-06,
            8.77960058e-01, 8.78447786e-01, 1.08487978e-03, 8.78247642e-01,
            8.49168618e-01, 3.95031998e-06, 5.97434109e-01, 1.28078037e-05,
            5.67592760e-01, 8.78206890e-01, 6.12286047e-01, 8.78779871e-01,
            8.77912398e-01, 1.83272157e-01, 8.72715441e-01, 8.78128291e-01,
            8.73939538e-01, 8.72556324e-01, 8.78911739e-01, 3.70369421e-03,
            2.44061741e-05, 8.77989870e-01, 8.34227230e-01, 7.74899792e-01,
            2.71975400e-02, 2.95818089e-01, 7.10097477e-02, 8.77974284e-01,
            3.52356854e-11, 8.45279787e-01, 8.70206365e-01, 8.17340660e-01,
            8.77616061e-01, 8.51336877e-01, 8.75768949e-01, 8.78233583e-01,
            8.74471206e-01, 5.86551670e-09, 5.83151275e-02, 1.69516576e-16,
            9.23145260e-10, 9.00806263e-03, 8.27869979e-01, 8.77344636e-01,
            2.00343171e-01, 8.78362775e-01, 8.77801947e-01, 8.60878918e-01,
            9.98846943e-03, 8.15347811e-01, 8.59636944e-01, 8.60093693e-01,
            8.77477616e-01, 1.01365891e-01, 3.67907715e-02, 8.78722364e-01,
            7.08345389e-01, 8.78467582e-01, 8.68460426e-01, 8.52745845e-01,
            4.37077951e-08, 7.69535071e-01, 8.77801587e-01, 1.09330350e-07,
            1.02316476e-12, 8.18531415e-01, 4.14396253e-03, 3.35283228e-02,
            8.60294568e-01, 1.19580120e-05, 8.24505904e-01, 8.17283660e-01,
            8.65410568e-01, 8.12520485e-01, 4.82188397e-18, 1.20493628e-05,
            4.77252385e-01, 7.95473908e-01, 8.13737089e-03, 8.50729080e-01,
            5.69537360e-01, 8.78314812e-01, 2.75589203e-10, 6.74327870e-01,
            1.26535694e-05, 8.74881990e-01, 8.31908536e-01, 3.79366406e-01,
            1.16304104e-03, 8.13343718e-01, 8.78406678e-01, 7.24114573e-01,
            8.71208193e-01, 6.33265654e-01, 2.29929444e-09, 2.56983792e-06,
            8.78222529e-01, 1.97272298e-03, 2.01489311e-01, 8.67843830e-01,
            8.42161052e-01, 8.69779112e-01, 2.06274228e-01, 8.78818498e-01,
            5.74141162e-07, 2.46340692e-09, 7.51439476e-01, 8.62134902e-01,
            8.77008390e-01, 1.41267877e-01, 8.58449226e-01, 5.65881130e-05,
            1.84449119e-01, 4.72362024e-06, 8.67755757e-01, 2.85519636e-01,
            2.25701477e-02, 2.91250605e-01, 8.35577145e-05, 7.83079493e-01,
            4.30441985e-01, 4.26989724e-06, 2.79831236e-05, 8.64225399e-01,
            4.15737011e-05, 8.33870987e-06, 7.16199679e-02]])




```python
# Ajustamos o shape em treino
y_pred_treino = y_pred_treino.reshape(-1)
y_treino = Y_treino.reshape(-1)
```


```python
# Convertemos as previs√µes para o valor bin√°rio de classe
# (0 ou 1, usando como threshold o valor de 0.5 da probablidade)
y_pred_treino = 1 * (y_pred_treino > 0.5)
```


```python
# Calculamos a acur√°cia comparando valor real com valor previsto
acc_treino = sum(1 * (y_pred_treino == y_treino)) / len(y_pred_treino) * 100
```


```python
print("Acur√°cia nos dados de treino: " + str(acc_treino) + " %")
```

    Acur√°cia nos dados de treino: 91.92546583850931 %



```python
print(classification_report(y_treino, y_pred_treino, target_names = ['Maligno','Banigno']))
```

                  precision    recall  f1-score   support
    
         Maligno       0.90      0.88      0.89       183
         Banigno       0.93      0.94      0.94       300
    
        accuracy                           0.92       483
       macro avg       0.92      0.91      0.91       483
    weighted avg       0.92      0.92      0.92       483


‚Äã    


```python
# Previs√µes com modelo usando dados de teste
y_pred_teste = predict(X_teste, parametros)
```


```python
# Visualiza os dados
y_pred_teste
```




    array([[8.70914083e-01, 8.60626952e-01, 8.73825229e-01, 1.18417887e-16,
            6.26854895e-03, 8.69472924e-01, 8.78308280e-01, 5.91224976e-01,
            8.13471757e-01, 8.67084928e-01, 8.77326855e-01, 1.96437938e-02,
            8.78205690e-01, 8.77980212e-01, 8.77937786e-01, 8.78700654e-01,
            9.05840601e-03, 8.77794699e-01, 7.56464066e-01, 8.63726966e-01,
            8.27578323e-01, 5.50158174e-01, 8.77947389e-01, 8.67900473e-01,
            5.95656403e-01, 5.27605690e-03, 4.68067870e-17, 8.78310301e-01,
            8.78583834e-01, 8.78398628e-01, 8.77739880e-01, 8.78473505e-01,
            2.75596997e-01, 7.51230818e-03, 8.68558960e-01, 8.34405421e-01,
            4.03931504e-05, 6.09434878e-01, 7.85844664e-01, 1.50489900e-03,
            1.45927902e-08, 8.78363623e-01, 8.59034870e-01, 1.70833078e-01,
            8.78455611e-01, 2.25544460e-02, 8.56791593e-01, 8.57051044e-01,
            8.77671342e-01, 6.41919591e-01, 1.27329412e-15, 1.29279919e-05,
            6.82249589e-01, 1.82319893e-02, 8.68206339e-01, 2.34669269e-02,
            7.27980711e-01, 4.10847565e-03, 8.78183775e-01, 4.12339354e-01,
            8.78903128e-01, 6.87899053e-01, 4.21148489e-01, 8.77199885e-01,
            7.95790549e-01, 1.13042635e-03, 4.46645541e-06, 8.77567961e-01,
            8.63181149e-01, 8.38640885e-01, 1.98442748e-02, 9.16646182e-03,
            8.77573298e-01, 8.08561826e-01, 8.66753421e-01, 6.03333368e-05,
            3.44789722e-04, 2.06796313e-23, 2.09417392e-04, 8.77843448e-01,
            8.77794471e-01, 1.57194508e-04, 7.71690384e-01, 8.77373847e-01,
            8.77002104e-01, 8.77806728e-01]])




```python
# Ajustamos os shapes
y_pred_teste = y_pred_teste.reshape(-1)
y_teste = Y_teste.reshape(-1)
```


```python
# Convertemos as previs√µes para o valor bin√°rio de classe
y_pred_teste = 1 * (y_pred_teste > 0.5)
```


```python
# Visualizamos as previs√µes
y_pred_teste
```




    array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
           1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
           1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
           0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1])




```python
# Calculamos a acur√°cia
acuracia = sum(1 * (y_pred_teste == y_teste)) / len(y_pred_teste) *100
```


```python
print("Acur√°cia nos dados de teste: " + str(acuracia) + " %")
```

    Acur√°cia nos dados de teste: 95.34883720930233 %



```python
print(classification_report(y_teste, y_pred_teste, target_names = ['Maligno','Benigno']))
```

                  precision    recall  f1-score   support
    
         Maligno       0.93      0.93      0.93        29
         Benigno       0.96      0.96      0.96        57
    
        accuracy                           0.95        86
       macro avg       0.95      0.95      0.95        86
    weighted avg       0.95      0.95      0.95        86


‚Äã    
